# 🎯 全栈数据工程师学习路线

> **目标**：7 年前端 + 1 年 Node.js → 全栈 + 数据处理工程师
>
> **周期**：12 个月（2024-2025）
>
> **每日投入**：1-2 小时

---

## 📊 技能雷达

| 技能              | 当前 | 目标 | 状态      |
| ----------------- | ---- | ---- | --------- |
| 前端              | 90%  | 90%  | ✅ 已掌握 |
| Node.js/后端框架  | 70%  | 85%  | 🔄 需深入 |
| Python 后端       | 35%  | 80%  | 🔄 进行中 |
| SQL/数据库        | 50%  | 85%  | 🔜 待加强 |
| 缓存/Redis        | 30%  | 75%  | 🔜 待加强 |
| 安全性            | 40%  | 80%  | 🔜 待补充 |
| 数据处理 (Pandas) | 15%  | 80%  | 🔜 待学习 |
| PySpark (批处理)  | 5%   | 75%  | 🔜 待学习 |
| PyFlink (流处理)  | 5%   | 70%  | 🔜 待学习 |
| AWS               | 60%  | 80%  | ✅ 有证书 |
| DevOps            | 30%  | 60%  | 🔜 待补充 |

---

## 📅 第 1 阶段：全栈基础（第 1-3 月）

### 🎯 目标：能独立开发完整后端项目，掌握后端核心内功

### Week 1-2：完成门店系统

- [ ] Flask 后端所有接口完成
- [ ] 前端对接完成
- [ ] 本地联调通过

### Week 3-4：SQL 深入 ⭐ 核心内功

- [ ] 复习 JOIN（INNER/LEFT/RIGHT）
- [ ] 学习子查询
- [ ] 学习窗口函数（ROW_NUMBER, RANK, LAG/LEAD）
- [ ] 学习索引原理（B+树、聚簇索引 vs 非聚簇索引）
- [ ] 学习 EXPLAIN 分析执行计划
- [ ] 学习事务隔离级别
- [ ] 完成 20 道 LeetCode SQL 题

```sql
-- 窗口函数示例
SELECT
  product_name,
  amount,
  ROW_NUMBER() OVER (ORDER BY amount DESC) as rank,
  SUM(amount) OVER () as total
FROM orders;

-- EXPLAIN 分析
EXPLAIN SELECT * FROM orders WHERE customer_id = 123;
```

### Week 5-6：Redis ⭐ 核心内功

- [ ] 安装 Redis，学习基本命令
- [ ] 数据类型：String, List, Set, Hash, ZSet
- [ ] 实践：Session 存储
- [ ] 实践：缓存热点数据（Cache Aside 模式）
- [ ] 实践：排行榜功能（ZSet）
- [ ] 理解缓存问题：穿透、击穿、雪崩及解决方案
- [ ] 学习缓存更新策略

```bash
# Redis 安装
docker run -d -p 6379:6379 redis
```

```javascript
// Cache Aside 模式
async function getProduct(id) {
  // 1. 先查缓存
  let product = await redis.get(`product:${id}`);
  if (product) return JSON.parse(product);

  // 2. 缓存没有，查数据库
  product = await db.query('SELECT * FROM products WHERE id = ?', [id]);

  // 3. 写入缓存
  await redis.setex(`product:${id}`, 3600, JSON.stringify(product));
  return product;
}
```

### Week 7-8：API 设计与安全 ⭐ 核心内功

- [ ] RESTful 规范
- [ ] 认证方案：JWT、OAuth 2.0、Session
- [ ] 接口限流（令牌桶、漏桶算法）
- [ ] 错误处理规范
- [ ] API 文档（Swagger/OpenAPI）
- [ ] **安全防护**（重要！）
  - [ ] XSS 防护（输入过滤、输出编码）
  - [ ] SQL 注入防护（参数化查询）
  - [ ] CSRF 防护
  - [ ] 密码加密（bcrypt，绝不 MD5）
  - [ ] HTTPS 配置
  - [ ] 敏感数据脱敏

```javascript
// 限流中间件示例 (Express)
const rateLimit = require('express-rate-limit');
const limiter = rateLimit({
  windowMs: 15 * 60 * 1000, // 15分钟
  max: 100, // 每IP最多100次请求
});
app.use('/api/', limiter);
```

### Week 9-10：性能优化 ⭐ 核心内功

- [ ] N+1 查询问题识别与解决
- [ ] 数据库连接池配置
- [ ] 慢查询日志分析
- [ ] 接口响应时间优化
- [ ] 内存泄漏排查

```javascript
// N+1 问题示例
// ❌ 差：N+1 查询
const orders = await Order.findAll();
for (const order of orders) {
  order.customer = await Customer.findById(order.customerId); // N 次查询！
}

// ✅ 好：JOIN 或预加载
const orders = await Order.findAll({
  include: [Customer], // 一次查询
});
```

### Week 11-12：部署上线

- [ ] Docker 镜像优化（多阶段构建）
- [ ] CI/CD 流水线完善
- [ ] 域名 + HTTPS（Let's Encrypt）
- [ ] 日志收集（console→ 文件 →ELK）
- [ ] 监控告警基础（健康检查、错误告警）

### ✅ 阶段检查点

- [ ] 门店系统已上线可访问
- [ ] 能写复杂 SQL 查询 + 会用 EXPLAIN
- [ ] 项目集成了 Redis 缓存
- [ ] 理解常见安全问题和防护方案
- [ ] 有完整的 CI/CD

---

## 📅 第 2 阶段：数据处理（第 4-6 月）

### 🎯 目标：能用 Python 做数据分析

### Week 1-2：Pandas 基础

- [ ] 安装 Pandas、Jupyter Notebook
- [ ] 读写数据：CSV、Excel、SQL
- [ ] 数据筛选：loc、iloc、条件筛选
- [ ] 数据聚合：groupby、agg

```python
import pandas as pd

df = pd.read_csv('orders.csv')
df.groupby('product')['amount'].sum()
```

### Week 3-4：Pandas 进阶

- [ ] 数据合并：merge、concat
- [ ] 透视表：pivot_table
- [ ] 时间序列处理
- [ ] 缺失值处理

### Week 5-6：数据可视化

- [ ] Matplotlib 基础图表
- [ ] 柱状图、折线图、饼图
- [ ] Seaborn 美化
- [ ] ECharts 前端展示

### Week 7-8：ETL 实践

- [ ] 写数据清洗脚本
- [ ] 定时任务（Cron）
- [ ] 数据质量检查
- [ ] 日志记录

### Week 9-12：实战项目

- [ ] 门店销售分析报告
  - [ ] 月度销售趋势
  - [ ] 商品销量排行
  - [ ] 客户消费分析
  - [ ] 库存周转率

### ✅ 阶段检查点

- [ ] 能用 Pandas 处理数据
- [ ] 写过 3+ 个数据分析脚本
- [ ] 有可视化报表产出
- [ ] 理解 ETL 流程

---

## 📅 第 3 阶段：大数据入门（第 7-9 月）

### 🎯 目标：能用 PySpark/PyFlink 处理大规模数据

### Week 1-2：Kafka 消息队列

- [ ] 安装 Kafka（Docker）
- [ ] 理解核心概念：Topic、Partition、Consumer Group
- [ ] 生产者 Producer
- [ ] 消费者 Consumer
- [ ] Python 客户端：kafka-python

```bash
# Kafka 安装
docker-compose up -d  # 用 docker-compose 启动 Kafka
```

```python
# Python Kafka 生产者
from kafka import KafkaProducer
import json

producer = KafkaProducer(
    bootstrap_servers='localhost:9092',
    value_serializer=lambda v: json.dumps(v).encode('utf-8')
)

producer.send('orders', {'order_id': 1, 'amount': 99.5})
```

### Week 3-4：PySpark 基础 ⭐ 批处理核心

- [ ] 安装 PySpark：`pip install pyspark`
- [ ] 理解 Spark 架构：Driver、Executor、Partition
- [ ] RDD 基本操作（了解即可）
- [ ] **DataFrame API**（重点！和 Pandas 类似）
- [ ] Spark SQL（用 SQL 查询大数据）

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, sum, avg, count

# 创建 SparkSession
spark = SparkSession.builder \
    .appName("OrderAnalysis") \
    .getOrCreate()

# 读取数据
df = spark.read.csv('orders.csv', header=True, inferSchema=True)

# DataFrame 操作（类似 Pandas）
result = df.filter(col('amount') > 100) \
    .groupBy('product_id') \
    .agg(
        sum('amount').alias('total'),
        count('*').alias('order_count'),
        avg('amount').alias('avg_amount')
    ) \
    .orderBy(col('total').desc())

result.show()

# Spark SQL（可以直接写 SQL）
df.createOrReplaceTempView("orders")
spark.sql("""
    SELECT product_id, SUM(amount) as total
    FROM orders
    GROUP BY product_id
    ORDER BY total DESC
""").show()
```

### Week 5-6：PySpark 实战

- [ ] 处理百万级 CSV/Parquet 数据
- [ ] 读写 MySQL/Hive
- [ ] 性能调优基础：分区、缓存
- [ ] **AWS EMR/Glue 运行 PySpark**

```python
# 读写 MySQL
df = spark.read.format("jdbc") \
    .option("url", "jdbc:mysql://host:3306/db") \
    .option("dbtable", "orders") \
    .option("user", "root") \
    .option("password", "xxx") \
    .load()

# 写入 Parquet（列式存储，性能更好）
df.write.parquet("s3://bucket/orders/")
```

### Week 7-8：PyFlink 基础 ⭐ 流处理核心

- [ ] 安装 PyFlink：`pip install apache-flink`
- [ ] 理解流处理 vs 批处理
- [ ] DataStream API
- [ ] Window 窗口操作（滚动窗口、滑动窗口）
- [ ] State 状态管理
- [ ] Watermark 水位线（处理乱序数据）

```python
from pyflink.datastream import StreamExecutionEnvironment
from pyflink.datastream.functions import MapFunction

env = StreamExecutionEnvironment.get_execution_environment()

# 从 Kafka 读取实时数据
ds = env.from_collection([
    {'order_id': 1, 'amount': 100},
    {'order_id': 2, 'amount': 200},
])

# 实时计算
result = ds.map(lambda x: (x['order_id'], x['amount'] * 1.1))

result.print()
env.execute("RealTimeOrder")
```

### Week 9-10：PyFlink 进阶

- [ ] Checkpoint 容错机制
- [ ] 连接 Kafka Source/Sink
- [ ] Table API（SQL 风格）
- [ ] **AWS Kinesis Data Analytics**

```python
# Flink 窗口聚合示例
from pyflink.datastream.window import TumblingProcessingTimeWindows
from pyflink.common import Time

# 每 10 秒统计一次销售额
ds.key_by(lambda x: x['store_id']) \
    .window(TumblingProcessingTimeWindows.of(Time.seconds(10))) \
    .reduce(lambda a, b: {'store_id': a['store_id'],
                          'amount': a['amount'] + b['amount']})
```

### Week 11-12：实时项目

- [ ] 搭建完整实时数据管道
  ```
  订单事件 → Kafka → PyFlink → Redis → 前端实时看板
                        ↓
                       S3 (数据归档)
  ```
- [ ] 实时销售统计仪表盘
- [ ] 实时库存预警系统
- [ ] **AWS 部署**：Kinesis + Lambda 或 EMR Flink

### ✅ 阶段检查点

- [ ] 能用 PySpark 处理 GB 级数据
- [ ] 能用 PyFlink 处理实时流数据
- [ ] 理解窗口、状态、Watermark 概念
- [ ] 搭建过 Kafka + Flink 管道
- [ ] 有实时数据项目产出

### 📊 PySpark vs PyFlink 对比

| 特性     | PySpark       | PyFlink                |
| -------- | ------------- | ---------------------- |
| 定位     | 批处理为主    | 流处理为主             |
| 延迟     | 秒~分钟级     | 毫秒~秒级              |
| API      | DataFrame/SQL | DataStream/Table       |
| 适用场景 | 离线分析、ETL | 实时监控、实时计算     |
| AWS 服务 | EMR、Glue     | Kinesis Data Analytics |
| 学习曲线 | ⭐⭐          | ⭐⭐⭐                 |

**建议**：先学 PySpark（API 和 Pandas 类似），再学 PyFlink（概念更复杂）

---

## 📅 第 4 阶段：进阶整合（第 10-12 月）

### 🎯 目标：具备数据平台设计能力 + 架构思维

### Week 1-2：后端架构进阶 ⭐ 高级选修

- [ ] 微服务概念理解（不急着用，先理解边界划分）
- [ ] 消息队列实践（RabbitMQ/Kafka）
  - [ ] 解耦场景：下单 → 发消息 → 发短信/扣库存
  - [ ] 削峰场景：秒杀请求先进队列
- [ ] 分布式基础概念
  - [ ] CAP 定理
  - [ ] 最终一致性
- [ ] 读写分离（MySQL 主从）

```javascript
// 消息队列解耦示例
// 下单时，不直接调用短信服务，而是发消息
await orderService.create(order);
await messageQueue.send('order.created', { orderId: order.id });

// 短信服务监听消息
messageQueue.subscribe('order.created', async (msg) => {
  await smsService.sendOrderNotification(msg.orderId);
});
```

### Week 3-4：数据仓库

- [ ] 理解 OLTP vs OLAP
- [ ] 维度建模（星型/雪花）
- [ ] 事实表/维度表设计
- [ ] Hive 基础使用
- [ ] 数仓分层（ODS → DWD → DWS → ADS）

### Week 5-8：云服务

- [ ] 阿里云/AWS 账号
- [ ] 对象存储（OSS/S3）
- [ ] 云数据库
- [ ] 大数据产品体验（DataWorks/EMR）

### Week 9-12：综合项目

- [ ] 完整数据平台
  ```
  数据采集 → 数据存储 → 数据处理 → 数据服务 → 数据展示
  ```
- [ ] 整理作品集
- [ ] 更新简历

### ✅ 阶段检查点

- [ ] 能画数据架构图
- [ ] 理解数仓分层
- [ ] 会用云服务
- [ ] 理解消息队列使用场景
- [ ] 有完整项目作品

---

## 🔧 后端内功清单（Express/NestJS 经验加成）

> 框架会变，内功通用！这些知识跨语言、跨框架都适用

### ✅ 已掌握（无需重复学）

| 技能        | 说明                              |
| ----------- | --------------------------------- |
| 路由/控制器 | Express Router, NestJS Controller |
| 中间件      | Middleware 链式调用               |
| 依赖注入    | NestJS DI 容器                    |
| 请求处理    | Body、Query、Params 解析          |
| ORM 使用    | TypeORM 实体、关联关系            |
| JWT 认证    | @nestjs/jwt, passport             |
| 基础部署    | Docker, CI/CD                     |

### 🔜 需要补充（核心内功）

| 技能           | 优先级 | 学习周期 | 备注                  |
| -------------- | ------ | -------- | --------------------- |
| 数据库索引原理 | ⭐⭐⭐ | 1-2 周   | B+树、EXPLAIN         |
| 缓存策略       | ⭐⭐⭐ | 1-2 周   | Cache Aside、更新策略 |
| 安全防护       | ⭐⭐⭐ | 1 周     | XSS/SQL 注入/CSRF     |
| N+1 问题       | ⭐⭐⭐ | 2 天     | 预加载、JOIN          |
| 事务隔离级别   | ⭐⭐   | 3 天     | 脏读、幻读            |
| 限流算法       | ⭐⭐   | 2 天     | 令牌桶、漏桶          |
| 日志规范       | ⭐⭐   | 2 天     | 结构化日志、等级      |

### 📌 进阶选修（按需学习）

| 技能       | 场景         | 何时学           |
| ---------- | ------------ | ---------------- |
| 消息队列   | 解耦、削峰   | 遇到复杂业务时   |
| 微服务拆分 | 大型项目     | 单体遇到瓶颈时   |
| 读写分离   | 高并发       | 数据库成为瓶颈时 |
| GraphQL    | 复杂前端需求 | REST 不够灵活时  |

---

## 📚 学习资源

### 书籍

| 书名                         | 对应阶段                |
| ---------------------------- | ----------------------- |
| 《SQL 必知必会》             | 第 1 阶段               |
| 《Redis 设计与实现》         | 第 1 阶段               |
| 《高性能 MySQL》             | 第 1 阶段（进阶）       |
| 《利用 Python 进行数据分析》 | 第 2 阶段               |
| 《Spark 权威指南》           | 第 3 阶段（PySpark）    |
| 《Flink 原理与实践》         | 第 3 阶段（PyFlink）    |
| 《数据密集型应用设计》       | 第 4 阶段（强烈推荐！） |

### 在线资源

| 资源                 | 链接                                                  |
| -------------------- | ----------------------------------------------------- |
| LeetCode SQL         | https://leetcode.cn/problemset/database/              |
| Pandas 官方文档      | https://pandas.pydata.org/docs/                       |
| **PySpark 官方文档** | https://spark.apache.org/docs/latest/api/python/      |
| **PyFlink 官方文档** | https://nightlies.apache.org/flink/flink-docs-stable/ |
| AWS Glue 文档        | https://docs.aws.amazon.com/glue/                     |
| AWS EMR 文档         | https://docs.aws.amazon.com/emr/                      |
| Boto3 SDK 文档       | https://boto3.amazonaws.com/v1/documentation/         |

### 练习平台

- LeetCode（SQL 题）
- Kaggle（数据分析项目）
- **Databricks Community**（免费 Spark 环境，强烈推荐！）
- **AWS Free Tier**（EMR/Glue 有免费额度）

---

## 🗂️ 项目清单

| 项目               | 阶段      | 技术栈              | 状态      |
| ------------------ | --------- | ------------------- | --------- |
| 门店管理系统       | 第 1 阶段 | Flask + MySQL       | 🔄 进行中 |
| 销售数据分析报告   | 第 2 阶段 | Pandas + Matplotlib | 🔜 待开始 |
| PySpark 批处理练习 | 第 3 阶段 | PySpark + S3        | 🔜 待开始 |
| PyFlink 练习项目   | 第 3 阶段 | PyFlink + Kafka     | 🔄 已创建 |
| 实时销售看板       | 第 3 阶段 | Flink + Redis + Vue | 🔜 待开始 |
| 完整数据平台       | 第 4 阶段 | AWS EMR/Glue        | 🔜 待开始 |

---

## 📍 项目路径

```
/Users/qiaozhen/app/
├── shop/
│   ├── shop-be/          # Flask 后端（当前项目）
│   └── shop-fe/          # 前端（如果有）
├── flink-demo/           # PyFlink 练习项目（已创建）
├── spark-demo/           # PySpark 练习项目（待创建）
├── data-analysis/        # Pandas 数据分析项目（待创建）
└── data-platform/        # AWS 数据平台项目（待创建）
```

---

## 📝 每日习惯

```
□ 早上 30 分钟：LeetCode SQL 题 / 技术文章
□ 晚上 1 小时：项目开发 / 技术学习
□ 周末 3 小时：系统学习 / 看书
```

---

## 💡 核心原则

1. **不贪多**：按顺序来，不要同时学太多
2. **重实践**：每学一个技术，必须有项目产出
3. **记笔记**：遇到的问题和解决方案记录下来
4. **坚持**：每天 1-2 小时，比周末突击 10 小时更有效
5. **内功优先**：框架 API 会忘，但 SQL/缓存/安全是通用技能，优先投入
6. **按需学习**：微服务/消息队列等高级主题，遇到问题再学，不提前焦虑

---

## 🏆 里程碑

- [ ] **3 个月**：门店系统上线，SQL 熟练
- [ ] **6 个月**：能做数据分析，会写 ETL
- [ ] **9 个月**：会用 Spark/Flink，有实时项目
- [ ] **12 个月**：全栈数据工程师，简历闪亮 ✨

---

> 最后更新：2024 年 12 月
>
> 加油！💪
